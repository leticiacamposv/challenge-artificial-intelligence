{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafio +A Educação - Engenheiro de Inteligência Artificial\n",
    "\n",
    "Este notebook contempla as etapas e soluções requeridas no desafio pontuadas no README.md na pasta raiz e estruturadas de acordo\n",
    "\n",
    "*Autora*: Leticia Campos Valente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 1: Indexação dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A autora optou por utilizar a ferramenta 'Elastic Search' pelo suporte aos formatos e tipos de dados presentes no desafio, como sua eficiência e velocidade nas pesquisas, além de lidar bem com grandes volumes de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Instanciando o Elasticsearch\n",
    "# es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É preciso primeiramente extrair as palavras chaves do texto para indexação conforme conteúdo e outras palavras chaves. Para o desafio, testa-se duas abordagens de extração de palavras chaves: 1 pelo método de Term Frequency-Inverse Document Frequency (TF-IDF) com NLP simples e 2 Utilizando um modelo pré-treinado da hugging face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Extração de palavras chaves por TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\letic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\letic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_keyword_extraction(text, top_n=10):\n",
    "    \"\"\"\n",
    "    Extrai as principais palavras-chave de um texto usando o método TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "    Parâmetros:\n",
    "    text (str)-> Texto em formato str para extração das palavras chaves\n",
    "    top_n (int) -> Número de palavras-chave a serem retornadas\n",
    "\n",
    "    Retorno: \n",
    "    list -> Lista das palavras-chave extraídas do texto, ordenadas por relevância\n",
    "    \"\"\"\n",
    "\n",
    "    stop_words = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "    # Pré-processamento\n",
    "    vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=top_n)\n",
    "\n",
    "    # Cálculo da matriz\n",
    "    tfidf_matrix = vectorizer.fit_transform([text])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    tfidf_scores = tfidf_matrix.toarray().flatten()\n",
    "\n",
    "    # Pós-processamento\n",
    "    keywords = sorted(zip(feature_names, tfidf_scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print('\\nWord indexes:')\n",
    "    print(vectorizer.vocabulary_)\n",
    "    \n",
    "    # display tf-idf values\n",
    "    print('\\ntf-idf value:')\n",
    "    print(tfidf_matrix)\n",
    "    \n",
    "    # in matrix form\n",
    "    print('\\ntf-idf values in matrix form:')\n",
    "    print(tfidf_matrix.toarray())\n",
    "    return [palavra for palavra, score in keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../resources/Apresentação.txt', 'r') as text_file:\n",
    "    text_data = text_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word indexes:\n",
      "{'web': 9, 'pã': 6, 'ginas': 3, 'html5': 4, 'estrutura': 0, 'formataã': 1, 'texto': 8, 'listas': 5, 'tabelas': 7, 'gina': 2}\n",
      "\n",
      "tf-idf value:\n",
      "  (0, 9)\t0.5163977794943223\n",
      "  (0, 6)\t0.5163977794943223\n",
      "  (0, 3)\t0.25819888974716115\n",
      "  (0, 4)\t0.43033148291193524\n",
      "  (0, 0)\t0.17213259316477408\n",
      "  (0, 1)\t0.17213259316477408\n",
      "  (0, 8)\t0.17213259316477408\n",
      "  (0, 5)\t0.17213259316477408\n",
      "  (0, 7)\t0.17213259316477408\n",
      "  (0, 2)\t0.25819888974716115\n",
      "\n",
      "tf-idf values in matrix form:\n",
      "[[0.17213259 0.17213259 0.25819889 0.25819889 0.43033148 0.17213259\n",
      "  0.51639778 0.17213259 0.17213259 0.51639778]]\n",
      "['pã', 'web', 'html5', 'gina', 'ginas', 'estrutura', 'formataã', 'listas', 'tabelas', 'texto']\n"
     ]
    }
   ],
   "source": [
    "text_kw = text_keyword_extraction(text_data, 10)\n",
    "print(text_kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['possível criar páginas web', 'plataforma web utilizam', 'final desta unidade', 'páginas web', 'páginas estáticas', 'página web', 'página web', 'página web', 'nesta unidade', 'vai estudar', 'sistemas desenvolvidos', 'seguintes aprendizados', 'novas marcações', 'linguagem html', 'documentos hipertexto', 'diversos recursos', 'dispositivos próprios', 'deve apresentar', 'deficientes visuais', 'busca automática', 'bons estudos', 'desenvolver listas', 'aplicar formatação', 'listas', 'formatação', 'uso', 'texto', 'texto', 'tanto', 'tabelas', 'tabelas', 'semântica', 'pesquisa', 'permitem', 'motores', 'meio', 'links', 'html5', 'html5', 'html5', 'html5', 'html5', 'facilitando', 'exibição', 'estrutura', 'estrutura', 'dinâmicas', 'definir', 'dados', 'conteúdo', 'auditivos', 'apresentação', 'apresentação', 'aprendizagem', 'aprendizagem', 'acessibilidade']\n"
     ]
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "import nltk\n",
    "\n",
    "# Certifique-se de baixar os recursos necessários do NLTK\n",
    "\n",
    "# Texto de exemplo\n",
    "texto = \"\"\"\n",
    "Sistemas desenvolvidos para a plataforma web utilizam a linguagem HTML para a exibição de conteúdo, tanto para páginas estáticas como dinâmicas. Com HTML5 é possível criar páginas web com diversos recursos para a apresentação de dados por meio de novas marcações que permitem o uso de semântica e acessibilidade, facilitando a pesquisa por motores de busca automática e dispositivos próprios para deficientes visuais e auditivos.\n",
    "\n",
    "Nesta Unidade de Aprendizagem, você vai estudar a estrutura de páginas web, a formatação de texto em documentos hipertexto e a apresentação de links, listas e tabelas em HTML5.\n",
    "\n",
    "Bons estudos.\n",
    "\n",
    "Ao final desta Unidade de Aprendizagem, você deve apresentar os seguintes aprendizados:\n",
    "Definir a estrutura de uma página web com HTML5.\n",
    "Aplicar formatação de texto em uma página web com HTML5.\n",
    "Desenvolver listas e tabelas em uma página web com HTML5.\n",
    "\"\"\"\n",
    "\n",
    "# Função para extrair palavras-chave usando RAKE\n",
    "def extrair_palavras_chave_rake(texto):\n",
    "    rake = Rake(stopwords=nltk.corpus.stopwords.words('portuguese'))\n",
    "    rake.extract_keywords_from_text(texto)\n",
    "    palavras_chave = rake.get_ranked_phrases()\n",
    "    return palavras_chave\n",
    "\n",
    "# Extração de palavras-chave\n",
    "palavras_chave = extrair_palavras_chave_rake(texto)\n",
    "print(palavras_chave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Extração de palavras chaves por pré-trained BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Github Repos\\challenge-artificial-intelligence\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Github Repos\\challenge-artificial-intelligence\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\letic\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "d:\\Github Repos\\challenge-artificial-intelligence\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[0.0327954].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 55\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Calcular a similaridade de coseno entre cada palavra e o vetor médio do documento\u001b[39;00m\n\u001b[0;32m     54\u001b[0m doc_embedding \u001b[38;5;241m=\u001b[39m word_embeddings_filtered\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 55\u001b[0m similarity_scores \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdoc_embedding\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_embeddings_filtered\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Ordenar as palavras pelo score de similaridade\u001b[39;00m\n\u001b[0;32m     58\u001b[0m top_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "File \u001b[1;32md:\\Github Repos\\challenge-artificial-intelligence\\.venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32md:\\Github Repos\\challenge-artificial-intelligence\\.venv\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1668\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1624\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   1625\u001b[0m \n\u001b[0;32m   1626\u001b[0m \u001b[38;5;124;03mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1664\u001b[0m \u001b[38;5;124;03m       [0.57..., 0.81...]])\u001b[39;00m\n\u001b[0;32m   1665\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1666\u001b[0m \u001b[38;5;66;03m# to avoid recursive import\u001b[39;00m\n\u001b[1;32m-> 1668\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_pairwise_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1670\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m normalize(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m Y:\n",
      "File \u001b[1;32md:\\Github Repos\\challenge-artificial-intelligence\\.venv\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:174\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, ensure_2d, copy)\u001b[0m\n\u001b[0;32m    164\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    165\u001b[0m         X,\n\u001b[0;32m    166\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m         ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m    172\u001b[0m     )\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m     Y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    184\u001b[0m         Y,\n\u001b[0;32m    185\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    190\u001b[0m         ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m    191\u001b[0m     )\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m precomputed:\n",
      "File \u001b[1;32md:\\Github Repos\\challenge-artificial-intelligence\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:1045\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1038\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1039\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1040\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1041\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1042\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1043\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1044\u001b[0m             )\n\u001b[1;32m-> 1045\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1048\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1049\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1050\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1051\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[0.0327954].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Carregar o modelo BERT pré-treinado e o tokenizer\n",
    "model_name = 'bert-base-multilingual-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Texto de exemplo\n",
    "texto = \"\"\"\n",
    "Sistemas desenvolvidos para a plataforma web utilizam a linguagem HTML para a exibição de conteúdo, tanto para páginas estáticas como dinâmicas. Com HTML5 é possível criar páginas web com diversos recursos para a apresentação de dados por meio de novas marcações que permitem o uso de semântica e acessibilidade, facilitando a pesquisa por motores de busca automática e dispositivos próprios para deficientes visuais e auditivos.\n",
    "\n",
    "Nesta Unidade de Aprendizagem, você vai estudar a estrutura de páginas web, a formatação de texto em documentos hipertexto e a apresentação de links, listas e tabelas em HTML5.\n",
    "\n",
    "Bons estudos.\n",
    "\n",
    "Ao final desta Unidade de Aprendizagem, você deve apresentar os seguintes aprendizados:\n",
    "Definir a estrutura de uma página web com HTML5.\n",
    "Aplicar formatação de texto em uma página web com HTML5.\n",
    "Desenvolver listas e tabelas em uma página web com HTML5.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenizar o texto\n",
    "inputs = tokenizer(texto, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "input_ids = inputs['input_ids']\n",
    "\n",
    "# Obter as embeddings do BERT\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extração das embeddings da última camada\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "# Calcular a média das embeddings das palavras\n",
    "word_embeddings = last_hidden_states.squeeze().mean(dim=0)\n",
    "\n",
    "# Tokenizar novamente para obter as palavras correspondentes\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
    "\n",
    "# Filtrar as palavras relevantes (remover tokens especiais como [CLS] e [SEP])\n",
    "word_embeddings_filtered = []\n",
    "tokens_filtered = []\n",
    "for token, embedding in zip(tokens, word_embeddings):\n",
    "    if token not in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]:\n",
    "        tokens_filtered.append(token)\n",
    "        word_embeddings_filtered.append(embedding)\n",
    "\n",
    "# Converter listas para arrays do NumPy para cálculo de similaridade\n",
    "word_embeddings_filtered = np.array([embedding.numpy() for embedding in word_embeddings_filtered])\n",
    "\n",
    "# Calcular a similaridade de coseno entre cada palavra e o vetor médio do documento\n",
    "doc_embedding = word_embeddings_filtered.mean(axis=0)\n",
    "similarity_scores = cosine_similarity([doc_embedding], word_embeddings_filtered).flatten()\n",
    "\n",
    "# Ordenar as palavras pelo score de similaridade\n",
    "top_n = 10\n",
    "top_indices = similarity_scores.argsort()[-top_n:][::-1]\n",
    "palavras_chave = [tokens_filtered[idx] for idx in top_indices]\n",
    "\n",
    "print(palavras_chave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1.1.1 Extração de palavras chaves por TF-IDF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
